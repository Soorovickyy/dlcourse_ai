{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([84, 72, 69])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-4ab6e7a8ed1f>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "<ipython-input-4-4ab6e7a8ed1f>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9.0000600001, array([6.00002]))\n",
      "(8.999940000099999, array([5.99998]))\n",
      "6.0\n",
      "6.000000000039306\n",
      "Gradient check passed!\n",
      "(5.00001, array([1., 1.]))\n",
      "(4.99999, array([1., 1.]))\n",
      "1.0\n",
      "0.9999999999621422\n",
      "(5.00001, array([1., 1.]))\n",
      "(4.99999, array([1., 1.]))\n",
      "1.0\n",
      "0.9999999999621422\n",
      "Gradient check passed!\n",
      "(6.00001, array([[1., 1.],\n",
      "       [1., 1.]]))\n",
      "(5.99999, array([[1., 1.],\n",
      "       [1., 1.]]))\n",
      "1.0\n",
      "0.9999999999621422\n",
      "(6.00001, array([[1., 1.],\n",
      "       [1., 1.]]))\n",
      "(5.99999, array([[1., 1.],\n",
      "       [1., 1.]]))\n",
      "1.0\n",
      "0.9999999999621422\n",
      "(6.00001, array([[1., 1.],\n",
      "       [1., 1.]]))\n",
      "(5.99999, array([[1., 1.],\n",
      "       [1., 1.]]))\n",
      "1.0\n",
      "0.9999999999621422\n",
      "(6.00001, array([[1., 1.],\n",
      "       [1., 1.]]))\n",
      "(5.99999, array([[1., 1.],\n",
      "       [1., 1.]]))\n",
      "1.0\n",
      "0.9999999999621422\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "print(probs)\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "print(probs)\n",
    "assert np.isclose(probs[0], 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.50940412e-05, 0.00669254912, 0.993262357)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4.50940412e-05, 6.69254912e-03, 9.93262357e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9932623568433084"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.718281828**-0.006760443547121171"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-ea4b88edfaa3>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([1])), np.array([1, 0, 0]).reshape(1, 3), np.float)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ea4b88edfaa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO Implement combined function or softmax and cross entropy and produces gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dlcourse_ai/assignments/assignment1/gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[0;34m(f, x, delta, tol)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0morig_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]).reshape(1, 3), np.array([1]))\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([1])), np.array([1, 0, 0]).reshape(1, 3), np.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[3.57972628]]), array([[ 0.20603355,  0.56005164, -0.97211667,  0.20603148]]))\n",
      "(array([[3.57972216]]), array([[ 0.20603027,  0.56005395, -0.97211656,  0.20603233]]))\n",
      "0.20603190919001857\n",
      "[[0.20603191]]\n",
      "(array([[3.57972982]]), array([[ 0.20603076,  0.56005526, -0.97211677,  0.20603076]]))\n",
      "(array([[3.57971862]]), array([[ 0.20603306,  0.56005033, -0.97211646,  0.20603306]]))\n",
      "0.5600527948339517\n",
      "[[0.56005279]]\n",
      "(array([[3.5797145]]), array([[ 0.20603185,  0.56005264, -0.97211634,  0.20603185]]))\n",
      "(array([[3.57973394]]), array([[ 0.20603197,  0.56005295, -0.97211688,  0.20603197]]))\n",
      "-0.9721166132139888\n",
      "[[-0.97211661]]\n",
      "(array([[3.57972628]]), array([[ 0.20603148,  0.56005164, -0.97211667,  0.20603355]]))\n",
      "(array([[3.57972216]]), array([[ 0.20603233,  0.56005395, -0.97211656,  0.20603027]]))\n",
      "0.20603190919001857\n",
      "[[0.20603191]]\n",
      "Gradient check passed!\n",
      "(array([[1.38353545],\n",
      "       [1.2142833 ],\n",
      "       [3.88280282]]), array([[ 0.68145473,  0.03392729,  0.03392729, -0.74930932],\n",
      "       [ 0.10923177,  0.29692274,  0.29692274, -0.70307726],\n",
      "       [ 0.15216302,  0.41362198, -0.97940697,  0.41362198]]))\n",
      "(array([[1.38352182],\n",
      "       [1.2142833 ],\n",
      "       [3.88280282]]), array([[ 0.68145039,  0.03392776,  0.03392776, -0.7493059 ],\n",
      "       [ 0.10923177,  0.29692274,  0.29692274, -0.70307726],\n",
      "       [ 0.15216302,  0.41362198, -0.97940697,  0.41362198]]))\n",
      "0.6814525618085748\n",
      "[[0.68145256]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-760b6f0c9c0f>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "<ipython-input-11-760b6f0c9c0f>:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
      "<ipython-input-11-760b6f0c9c0f>:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "<ipython-input-11-760b6f0c9c0f>:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-760b6f0c9c0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#linear_classifer.softmax_with_cross_entropy(predictions, target_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dlcourse_ai/assignments/assignment1/gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[0;34m(f, x, delta, tol)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_grad_at_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# TODO compute value of numeric gradient of f to idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_grad_at_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalytic_grad_at_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradients are different at %s. Analytic: %2.5f, Numeric: %2.5f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalytic_grad_at_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_grad_at_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1 \n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "#linear_classifer.softmax_with_cross_entropy(predictions, target_index)\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "\n",
    "\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "#linear_classifer.softmax_with_cross_entropy(predictions, target_index)\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2.1269192 , 0.04858735]), array([[-0.88079603,  0.88079603],\n",
      "       [-0.83337015,  0.83337015],\n",
      "       [ 0.9282219 , -0.9282219 ]]))\n",
      "(array([2.12693682, 0.04858735]), array([[-0.88079813,  0.88079813],\n",
      "       [-0.83337225,  0.83337225],\n",
      "       [ 0.928224  , -0.928224  ]]))\n",
      "-0.8807970779778823\n",
      "[-0.88079708  0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-c7d5e1cf0271>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
      "<ipython-input-12-c7d5e1cf0271>:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
      "<ipython-input-12-c7d5e1cf0271>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.ones(batch_size, dtype=np.int)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c7d5e1cf0271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#print(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dlcourse_ai/assignments/assignment1/gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[0;34m(f, x, delta, tol)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_grad_at_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# TODO compute value of numeric gradient of f to idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_grad_at_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalytic_grad_at_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradients are different at %s. Analytic: %2.5f, Numeric: %2.5f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalytic_grad_at_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_grad_at_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "#print(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И теперь регуляризация\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sumij W[i, j]2\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.060000100000500003, array([[ 0.0100001,  0.02     ],\n",
      "       [-0.01     ,  0.01     ],\n",
      "       [ 0.01     ,  0.02     ]]))\n",
      "(0.0599999000005, array([[ 0.0099999,  0.02     ],\n",
      "       [-0.01     ,  0.01     ],\n",
      "       [ 0.01     ,  0.02     ]]))\n",
      "0.01\n",
      "0.010000000000287557\n",
      "(0.0600002000005, array([[ 0.01     ,  0.0200001],\n",
      "       [-0.01     ,  0.01     ],\n",
      "       [ 0.01     ,  0.02     ]]))\n",
      "(0.0599998000005, array([[ 0.01     ,  0.0199999],\n",
      "       [-0.01     ,  0.01     ],\n",
      "       [ 0.01     ,  0.02     ]]))\n",
      "0.02\n",
      "0.019999999999881224\n",
      "(0.0599999000005, array([[ 0.01     ,  0.02     ],\n",
      "       [-0.0099999,  0.01     ],\n",
      "       [ 0.01     ,  0.02     ]]))\n",
      "(0.060000100000500003, array([[ 0.01     ,  0.02     ],\n",
      "       [-0.0100001,  0.01     ],\n",
      "       [ 0.01     ,  0.02     ]]))\n",
      "-0.01\n",
      "-0.010000000000287557\n",
      "(0.060000100000500003, array([[ 0.01     ,  0.02     ],\n",
      "       [-0.01     ,  0.0100001],\n",
      "       [ 0.01     ,  0.02     ]]))\n",
      "(0.0599999000005, array([[ 0.01     ,  0.02     ],\n",
      "       [-0.01     ,  0.0099999],\n",
      "       [ 0.01     ,  0.02     ]]))\n",
      "0.01\n",
      "0.010000000000287557\n",
      "(0.060000100000500003, array([[ 0.01     ,  0.02     ],\n",
      "       [-0.01     ,  0.01     ],\n",
      "       [ 0.0100001,  0.02     ]]))\n",
      "(0.0599999000005, array([[ 0.01     ,  0.02     ],\n",
      "       [-0.01     ,  0.01     ],\n",
      "       [ 0.0099999,  0.02     ]]))\n",
      "0.01\n",
      "0.010000000000287557\n",
      "(0.0600002000005, array([[ 0.01     ,  0.02     ],\n",
      "       [-0.01     ,  0.01     ],\n",
      "       [ 0.01     ,  0.0200001]]))\n",
      "(0.0599998000005, array([[ 0.01     ,  0.02     ],\n",
      "       [-0.01     ,  0.01     ],\n",
      "       [ 0.01     ,  0.0199999]]))\n",
      "0.02\n",
      "0.019999999999881224\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)\n",
    "#print(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((3,2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2], [3, 4], [5, 6], [7, 8]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2. 3. 4. 5.]\n",
      "  [6. 7. 8. 9.]]\n",
      "\n",
      " [[2. 3. 4. 5.]\n",
      "  [6. 7. 8. 9.]]\n",
      "\n",
      " [[2. 3. 4. 5.]\n",
      "  [6. 7. 8. 9.]]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    a[i, :, :] = np.ones((2, 4)) + np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "3073\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dimasyrovitsky/dlcourse_ai/assignments/assignment1/linear_classifer.py:40: RuntimeWarning: divide by zero encountered in log\n",
      "  return (- np.log(probs[target_index]))\n",
      "/Users/dimasyrovitsky/dlcourse_ai/assignments/assignment1/linear_classifer.py:227: RuntimeWarning: overflow encountered in double_scalars\n",
      "  loss[k] = cross_entropy_loss(sm[k], y[batches_indices[j]][k]) +  reg * np.sum(self.W**2)\n",
      "/Users/dimasyrovitsky/dlcourse_ai/assignments/assignment1/linear_classifer.py:227: RuntimeWarning: overflow encountered in square\n",
      "  loss[k] = cross_entropy_loss(sm[k], y[batches_indices[j]][k]) +  reg * np.sum(self.W**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3073, 10)\n",
      "[[[ 2.06233364e+207 -2.89270534e+207 -2.04443625e+207 ...\n",
      "   -1.09583961e+206 -2.21129209e+207  1.91068561e+207]\n",
      "  [ 1.65564196e+207  5.89118276e+207 -3.55602681e+207 ...\n",
      "   -2.33924276e+207  4.99081727e+205  2.03409117e+207]\n",
      "  [ 8.91194101e+206  2.64265123e+206 -8.46839611e+206 ...\n",
      "   -1.41131550e+207 -3.05344169e+207  2.58066933e+205]\n",
      "  ...\n",
      "  [ 1.70326278e+207  2.98870641e+207  8.72500434e+206 ...\n",
      "   -2.63092226e+207 -4.10313384e+207 -2.85526166e+207]\n",
      "  [ 2.10876501e+207  5.46219828e+207  9.69516340e+205 ...\n",
      "   -4.21859531e+207  1.34230780e+207  4.97228999e+207]\n",
      "  [-4.23614150e+207  1.60952527e+208  1.04150295e+208 ...\n",
      "   -5.54965370e+207 -6.07698667e+207 -2.12790098e+207]]\n",
      "\n",
      " [[ 2.06233364e+207 -2.89270534e+207 -2.04443625e+207 ...\n",
      "   -1.09583961e+206 -2.21129209e+207  1.91068561e+207]\n",
      "  [ 1.65564196e+207  5.89118276e+207 -3.55602681e+207 ...\n",
      "   -2.33924276e+207  4.99081727e+205  2.03409117e+207]\n",
      "  [ 8.91194101e+206  2.64265123e+206 -8.46839611e+206 ...\n",
      "   -1.41131550e+207 -3.05344169e+207  2.58066933e+205]\n",
      "  ...\n",
      "  [ 1.70326278e+207  2.98870641e+207  8.72500434e+206 ...\n",
      "   -2.63092226e+207 -4.10313384e+207 -2.85526166e+207]\n",
      "  [ 2.10876501e+207  5.46219828e+207  9.69516340e+205 ...\n",
      "   -4.21859531e+207  1.34230780e+207  4.97228999e+207]\n",
      "  [-4.23614150e+207  1.60952527e+208  1.04150295e+208 ...\n",
      "   -5.54965370e+207 -6.07698667e+207 -2.12790098e+207]]\n",
      "\n",
      " [[ 2.06233364e+207 -2.89270534e+207 -2.04443625e+207 ...\n",
      "   -1.09583961e+206 -2.21129209e+207  1.91068561e+207]\n",
      "  [ 1.65564196e+207  5.89118276e+207 -3.55602681e+207 ...\n",
      "   -2.33924276e+207  4.99081727e+205  2.03409117e+207]\n",
      "  [ 8.91194101e+206  2.64265123e+206 -8.46839611e+206 ...\n",
      "   -1.41131550e+207 -3.05344169e+207  2.58066933e+205]\n",
      "  ...\n",
      "  [ 1.70326278e+207  2.98870641e+207  8.72500434e+206 ...\n",
      "   -2.63092226e+207 -4.10313384e+207 -2.85526166e+207]\n",
      "  [ 2.10876501e+207  5.46219828e+207  9.69516340e+205 ...\n",
      "   -4.21859531e+207  1.34230780e+207  4.97228999e+207]\n",
      "  [-4.23614150e+207  1.60952527e+208  1.04150295e+208 ...\n",
      "   -5.54965370e+207 -6.07698667e+207 -2.12790098e+207]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.06233364e+207 -2.89270534e+207 -2.04443625e+207 ...\n",
      "   -1.09583961e+206 -2.21129209e+207  1.91068561e+207]\n",
      "  [ 1.65564196e+207  5.89118276e+207 -3.55602681e+207 ...\n",
      "   -2.33924276e+207  4.99081727e+205  2.03409117e+207]\n",
      "  [ 8.91194101e+206  2.64265123e+206 -8.46839611e+206 ...\n",
      "   -1.41131550e+207 -3.05344169e+207  2.58066933e+205]\n",
      "  ...\n",
      "  [ 1.70326278e+207  2.98870641e+207  8.72500434e+206 ...\n",
      "   -2.63092226e+207 -4.10313384e+207 -2.85526166e+207]\n",
      "  [ 2.10876501e+207  5.46219828e+207  9.69516340e+205 ...\n",
      "   -4.21859531e+207  1.34230780e+207  4.97228999e+207]\n",
      "  [-4.23614150e+207  1.60952527e+208  1.04150295e+208 ...\n",
      "   -5.54965370e+207 -6.07698667e+207 -2.12790098e+207]]\n",
      "\n",
      " [[ 2.06233364e+207 -2.89270534e+207 -2.04443625e+207 ...\n",
      "   -1.09583961e+206 -2.21129209e+207  1.91068561e+207]\n",
      "  [ 1.65564196e+207  5.89118276e+207 -3.55602681e+207 ...\n",
      "   -2.33924276e+207  4.99081727e+205  2.03409117e+207]\n",
      "  [ 8.91194101e+206  2.64265123e+206 -8.46839611e+206 ...\n",
      "   -1.41131550e+207 -3.05344169e+207  2.58066933e+205]\n",
      "  ...\n",
      "  [ 1.70326278e+207  2.98870641e+207  8.72500434e+206 ...\n",
      "   -2.63092226e+207 -4.10313384e+207 -2.85526166e+207]\n",
      "  [ 2.10876501e+207  5.46219828e+207  9.69516340e+205 ...\n",
      "   -4.21859531e+207  1.34230780e+207  4.97228999e+207]\n",
      "  [-4.23614150e+207  1.60952527e+208  1.04150295e+208 ...\n",
      "   -5.54965370e+207 -6.07698667e+207 -2.12790098e+207]]\n",
      "\n",
      " [[ 2.06233364e+207 -2.89270534e+207 -2.04443625e+207 ...\n",
      "   -1.09583961e+206 -2.21129209e+207  1.91068561e+207]\n",
      "  [ 1.65564196e+207  5.89118276e+207 -3.55602681e+207 ...\n",
      "   -2.33924276e+207  4.99081727e+205  2.03409117e+207]\n",
      "  [ 8.91194101e+206  2.64265123e+206 -8.46839611e+206 ...\n",
      "   -1.41131550e+207 -3.05344169e+207  2.58066933e+205]\n",
      "  ...\n",
      "  [ 1.70326278e+207  2.98870641e+207  8.72500434e+206 ...\n",
      "   -2.63092226e+207 -4.10313384e+207 -2.85526166e+207]\n",
      "  [ 2.10876501e+207  5.46219828e+207  9.69516340e+205 ...\n",
      "   -4.21859531e+207  1.34230780e+207  4.97228999e+207]\n",
      "  [-4.23614150e+207  1.60952527e+208  1.04150295e+208 ...\n",
      "   -5.54965370e+207 -6.07698667e+207 -2.12790098e+207]]]\n",
      "(3073, 10)\n",
      "[[ 6.18700092e+209 -8.67811601e+209 -6.13330876e+209 ... -3.28751883e+208\n",
      "  -6.63387628e+209  5.73205683e+209]\n",
      " [ 4.96692588e+209  1.76735483e+210 -1.06680804e+210 ... -7.01772829e+209\n",
      "   1.49724518e+208  6.10227351e+209]\n",
      " [ 2.67358230e+209  7.92795368e+208 -2.54051883e+209 ... -4.23394649e+209\n",
      "  -9.16032507e+209  7.74200798e+207]\n",
      " ...\n",
      " [ 5.10978835e+209  8.96611924e+209  2.61750130e+209 ... -7.89276677e+209\n",
      "  -1.23094015e+210 -8.56578499e+209]\n",
      " [ 6.32629504e+209  1.63865949e+210  2.90854902e+208 ... -1.26557859e+210\n",
      "   4.02692339e+209  1.49168700e+210]\n",
      " [-1.27084245e+210  4.82857581e+210  3.12450885e+210 ... -1.66489611e+210\n",
      "  -1.82309600e+210 -6.38370295e+209]]\n",
      "(3073, 10)\n",
      "[[-5.15583410e+206  7.23176334e+206  5.11109063e+206 ...  2.73959902e+205\n",
      "   5.52823023e+206 -4.77671403e+206]\n",
      " [-4.13910490e+206 -1.47279569e+207  8.89006702e+206 ...  5.84810691e+206\n",
      "  -1.24770432e+205 -5.08522793e+206]\n",
      " [-2.22798525e+206 -6.60662807e+205  2.11709903e+206 ...  3.52828874e+206\n",
      "   7.63360422e+206 -6.45167332e+204]\n",
      " ...\n",
      " [-4.25815696e+206 -7.47176603e+206 -2.18125109e+206 ...  6.57730564e+206\n",
      "   1.02578346e+207  7.13815416e+206]\n",
      " [-5.27191253e+206 -1.36554957e+207 -2.42379085e+205 ...  1.05464883e+207\n",
      "  -3.35576949e+206 -1.24307250e+207]\n",
      " [ 1.05903538e+207 -4.02381318e+207 -2.60375738e+207 ...  1.38741343e+207\n",
      "   1.51924667e+207  5.31975246e+206]]\n",
      "[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe0f82f8be0>]"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000, 3073)\n",
      "[[-0.28583216 -0.30627255 -0.24730667 ...  0.12864196  0.12063333\n",
      "   1.        ]\n",
      " [-0.05053804 -0.00823333  0.01543843 ... -0.23606392 -0.23230784\n",
      "   1.        ]\n",
      " [-0.01524392 -0.01999804 -0.10220863 ...  0.38354392  0.39906471\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.09759686 -0.05137059 -0.07867922 ...  0.45805373  0.37945686\n",
      "   1.        ]\n",
      " [ 0.14554039  0.16823725  0.23112471 ... -0.19684824 -0.15779804\n",
      "   1.        ]\n",
      " [-0.19563608 -0.22784118 -0.15318902 ... -0.28312275 -0.20485686\n",
      "   1.        ]]\n",
      "Accuracy:  nan\n",
      "Epoch 0, loss: 2.302669\n",
      "Epoch 1, loss: 2.302346\n",
      "Epoch 2, loss: 2.303133\n",
      "Epoch 3, loss: 2.301955\n",
      "Epoch 4, loss: 2.302710\n",
      "Epoch 5, loss: 2.303081\n",
      "Epoch 6, loss: 2.302524\n",
      "Epoch 7, loss: 2.302395\n",
      "Epoch 8, loss: 2.303378\n",
      "Epoch 9, loss: 2.302639\n",
      "Epoch 10, loss: 2.302793\n",
      "Epoch 11, loss: 2.302409\n",
      "Epoch 12, loss: 2.303378\n",
      "Epoch 13, loss: 2.302859\n",
      "Epoch 14, loss: 2.303331\n",
      "Epoch 15, loss: 2.301604\n",
      "Epoch 16, loss: 2.301776\n",
      "Epoch 17, loss: 2.303080\n",
      "Epoch 18, loss: 2.302446\n",
      "Epoch 19, loss: 2.302220\n",
      "Epoch 20, loss: 2.301862\n",
      "Epoch 21, loss: 2.301958\n",
      "Epoch 22, loss: 2.302744\n",
      "Epoch 23, loss: 2.302651\n",
      "Epoch 24, loss: 2.302775\n",
      "Epoch 25, loss: 2.303696\n",
      "Epoch 26, loss: 2.304283\n",
      "Epoch 27, loss: 2.302291\n",
      "Epoch 28, loss: 2.302162\n",
      "Epoch 29, loss: 2.303208\n",
      "Epoch 30, loss: 2.303482\n",
      "Epoch 31, loss: 2.302748\n",
      "Epoch 32, loss: 2.302353\n",
      "Epoch 33, loss: 2.302537\n",
      "Epoch 34, loss: 2.303285\n",
      "Epoch 35, loss: 2.303168\n",
      "Epoch 36, loss: 2.301943\n",
      "Epoch 37, loss: 2.301855\n",
      "Epoch 38, loss: 2.303501\n",
      "Epoch 39, loss: 2.302600\n",
      "Epoch 40, loss: 2.302432\n",
      "Epoch 41, loss: 2.303405\n",
      "Epoch 42, loss: 2.302248\n",
      "Epoch 43, loss: 2.302877\n",
      "Epoch 44, loss: 2.302303\n",
      "Epoch 45, loss: 2.303139\n",
      "Epoch 46, loss: 2.303994\n",
      "Epoch 47, loss: 2.301744\n",
      "Epoch 48, loss: 2.303727\n",
      "Epoch 49, loss: 2.301947\n",
      "Epoch 50, loss: 2.303322\n",
      "Epoch 51, loss: 2.302930\n",
      "Epoch 52, loss: 2.303357\n",
      "Epoch 53, loss: 2.303479\n",
      "Epoch 54, loss: 2.302416\n",
      "Epoch 55, loss: 2.302719\n",
      "Epoch 56, loss: 2.302006\n",
      "Epoch 57, loss: 2.301994\n",
      "Epoch 58, loss: 2.301959\n",
      "Epoch 59, loss: 2.301329\n",
      "Epoch 60, loss: 2.302468\n",
      "Epoch 61, loss: 2.301858\n",
      "Epoch 62, loss: 2.304159\n",
      "Epoch 63, loss: 2.302928\n",
      "Epoch 64, loss: 2.302532\n",
      "Epoch 65, loss: 2.302894\n",
      "Epoch 66, loss: 2.302612\n",
      "Epoch 67, loss: 2.303449\n",
      "Epoch 68, loss: 2.302712\n",
      "Epoch 69, loss: 2.302624\n",
      "Epoch 70, loss: 2.303590\n",
      "Epoch 71, loss: 2.303118\n",
      "Epoch 72, loss: 2.302132\n",
      "Epoch 73, loss: 2.303166\n",
      "Epoch 74, loss: 2.303198\n",
      "Epoch 75, loss: 2.302796\n",
      "Epoch 76, loss: 2.302861\n",
      "Epoch 77, loss: 2.302986\n",
      "Epoch 78, loss: 2.303493\n",
      "Epoch 79, loss: 2.303097\n",
      "Epoch 80, loss: 2.302921\n",
      "Epoch 81, loss: 2.302075\n",
      "Epoch 82, loss: 2.301830\n",
      "Epoch 83, loss: 2.303083\n",
      "Epoch 84, loss: 2.303420\n",
      "Epoch 85, loss: 2.302515\n",
      "Epoch 86, loss: 2.302396\n",
      "Epoch 87, loss: 2.302508\n",
      "Epoch 88, loss: 2.302685\n",
      "Epoch 89, loss: 2.302694\n",
      "Epoch 90, loss: 2.303541\n",
      "Epoch 91, loss: 2.303189\n",
      "Epoch 92, loss: 2.302085\n",
      "Epoch 93, loss: 2.303097\n",
      "Epoch 94, loss: 2.302645\n",
      "Epoch 95, loss: 2.303342\n",
      "Epoch 96, loss: 2.302177\n",
      "Epoch 97, loss: 2.302075\n",
      "Epoch 98, loss: 2.303079\n",
      "Epoch 99, loss: 2.303053\n",
      "(1000,)\n",
      "Accuracy after training for 100 epochs:  nan\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(val_X.shape)\n",
    "print(val_X)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-609-9e0a7501a45c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# than provided initially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best validation accuracy achieved: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "best_classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3073)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-608-548fe2acbfe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Linear softmax classifier test set accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_X.shape)\n",
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
