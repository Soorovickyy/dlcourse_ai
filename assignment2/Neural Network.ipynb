{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 10]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[3:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c6/_zhl81td4_7dslytk2yh2p6c0000gn/T/ipykernel_80128/2810386256.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "/var/folders/c6/_zhl81td4_7dslytk2yh2p6c0000gn/T/ipykernel_80128/2810386256.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 3072)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.08975373, -0.03568431,  0.08994824, ..., -0.12654745,\n",
       "        -0.03998549,  0.09710392],\n",
       "       [ 0.01612863,  0.01921765,  0.05465412, ..., -0.15007686,\n",
       "        -0.13410314, -0.10681765],\n",
       "       [-0.25445961, -0.32980196, -0.36887529, ..., -0.16968471,\n",
       "        -0.20469137, -0.23230784],\n",
       "       ...,\n",
       "       [ 0.38083451,  0.36823725,  0.36053647, ...,  0.21462902,\n",
       "         0.23060275,  0.21867255],\n",
       "       [-0.2426949 , -0.22391961, -0.12965961, ..., -0.11478275,\n",
       "        -0.14194627, -0.13034706],\n",
       "       [ 0.01220706,  0.02706078,  0.18798745, ...,  0.09306039,\n",
       "         0.06981843,  0.10886863]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "[[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)\n",
    "#check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "[[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n",
      " [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]]\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "#check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "#check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')\n",
    "#check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8531916400000022e-07"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.23127706e-06 - 2.63515479e-06 - 4.82047374e-07 + 1.86871414e-06 - 1.43243511e-06 + 6.34965238e-07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "#loss = nn.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "#gradient_check.check_model_gradient(nn, train_X[:2], train_y[:2])\n",
    "\n",
    "# Работает только на одной выборке\n",
    "loss = model.compute_loss_and_gradients(train_X[:1], train_y[:1])\n",
    "check_model_gradient(model, train_X[:1], train_y[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:1], train_y[:1])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:1], train_y[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 3072)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.666424, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.386994, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.388711, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.652021, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.678596, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.720219, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.606955, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.614457, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.576355, Train accuracy: 0.099000, val accuracy: 0.093000\n",
      "Loss: 2.721337, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.467503, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.375576, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.607656, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.671210, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.535297, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.596641, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.548238, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.403254, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.637186, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.629690, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fadb04db310>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.085328, Train accuracy: 0.328667, val accuracy: 0.347000\n",
      "Loss: 7.748543, Train accuracy: 0.495333, val accuracy: 0.504000\n",
      "Loss: 5.300400, Train accuracy: 0.538111, val accuracy: 0.554000\n",
      "Loss: 8.190814, Train accuracy: 0.607667, val accuracy: 0.603000\n",
      "Loss: 6.934415, Train accuracy: 0.646333, val accuracy: 0.657000\n",
      "Loss: 7.310535, Train accuracy: 0.623889, val accuracy: 0.628000\n",
      "Loss: 9.124328, Train accuracy: 0.637000, val accuracy: 0.615000\n",
      "Loss: 7.801071, Train accuracy: 0.647000, val accuracy: 0.638000\n",
      "Loss: 8.410839, Train accuracy: 0.683000, val accuracy: 0.654000\n",
      "Loss: 9.102121, Train accuracy: 0.664333, val accuracy: 0.620000\n",
      "Loss: 7.713002, Train accuracy: 0.659556, val accuracy: 0.654000\n",
      "Loss: 9.170530, Train accuracy: 0.621778, val accuracy: 0.614000\n",
      "Loss: 8.521389, Train accuracy: 0.687556, val accuracy: 0.651000\n",
      "Loss: 8.092240, Train accuracy: 0.664222, val accuracy: 0.637000\n",
      "Loss: 8.001689, Train accuracy: 0.687889, val accuracy: 0.667000\n",
      "Loss: 8.988351, Train accuracy: 0.622778, val accuracy: 0.599000\n",
      "Loss: 10.969931, Train accuracy: 0.634556, val accuracy: 0.615000\n",
      "Loss: 10.853308, Train accuracy: 0.719111, val accuracy: 0.707000\n",
      "Loss: 8.511083, Train accuracy: 0.698556, val accuracy: 0.663000\n",
      "Loss: 9.431609, Train accuracy: 0.694111, val accuracy: 0.645000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fada0e240a0>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.417538, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.701758, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.496833, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.268660, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 1.131956, Train accuracy: 0.215333, val accuracy: 0.219000\n",
      "Loss: 2.328107, Train accuracy: 0.262222, val accuracy: 0.264000\n",
      "Loss: 1.483309, Train accuracy: 0.286222, val accuracy: 0.290000\n",
      "Loss: 2.455566, Train accuracy: 0.326556, val accuracy: 0.331000\n",
      "Loss: 2.879840, Train accuracy: 0.394111, val accuracy: 0.387000\n",
      "Loss: 3.553640, Train accuracy: 0.430778, val accuracy: 0.421000\n",
      "Loss: 3.788540, Train accuracy: 0.476778, val accuracy: 0.479000\n",
      "Loss: 2.194852, Train accuracy: 0.514333, val accuracy: 0.514000\n",
      "Loss: 2.681782, Train accuracy: 0.545667, val accuracy: 0.530000\n",
      "Loss: 4.034963, Train accuracy: 0.572778, val accuracy: 0.558000\n",
      "Loss: 4.449949, Train accuracy: 0.597889, val accuracy: 0.589000\n",
      "Loss: 3.503568, Train accuracy: 0.617333, val accuracy: 0.610000\n",
      "Loss: 5.130088, Train accuracy: 0.624111, val accuracy: 0.609000\n",
      "Loss: 3.457729, Train accuracy: 0.648889, val accuracy: 0.643000\n",
      "Loss: 4.806461, Train accuracy: 0.658778, val accuracy: 0.644000\n",
      "Loss: 4.606940, Train accuracy: 0.672000, val accuracy: 0.654000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.303200, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302127, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.303055, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: 2.302262, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: 2.302592, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: 2.302113, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: 2.302845, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: 2.301822, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: 2.302055, Train accuracy: 0.133333, val accuracy: 0.266667\n",
      "Loss: 2.301899, Train accuracy: 0.266667, val accuracy: 0.333333\n",
      "Loss: 2.302612, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302702, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301456, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301298, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301148, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302609, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300731, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300698, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300618, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300279, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.303135, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.299952, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301060, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.299678, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.303310, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.303990, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.303156, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.303206, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.303519, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301618, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.303355, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301513, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.303454, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.298355, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.297948, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300403, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.297728, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300363, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300252, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304638, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304737, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304737, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304208, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300064, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.296700, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304986, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.305035, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304200, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304481, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300565, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300512, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304625, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.305334, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304721, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300339, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300250, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304865, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304957, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300132, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304796, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.294140, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.299976, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.299884, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.293496, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.293709, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.305932, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.293401, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.305456, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.305441, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.292798, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.292650, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.292427, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.292525, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.291965, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.305806, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.299254, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.305906, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.298382, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.291638, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.298283, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.305841, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.306778, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.298050, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.290862, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.297950, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.306929, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.306979, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.298636, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.289865, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.298533, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.289867, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.289720, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.297553, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.297503, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.306538, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.298225, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.307477, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.298063, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.288629, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.306786, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.288196, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.307025, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.307824, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287376, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.297701, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287590, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.307975, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.296912, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287144, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.297442, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.286559, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.307505, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308322, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308371, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.297184, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.285809, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308521, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.297096, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.285085, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.285607, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308104, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308154, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308773, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.296720, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308304, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.296037, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.284479, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308180, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.296534, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308367, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.295674, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308380, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.295698, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.309321, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.295477, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.283032, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.295379, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308678, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.282033, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.309666, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.308894, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.282324, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.282178, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.281856, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.309027, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.280965, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310015, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.281517, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.281372, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.280355, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.309326, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310219, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.294594, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.280644, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.280499, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.295076, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.294542, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310518, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.294446, Train accuracy: 0.200000, val accuracy: 0.133333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.310618, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.309824, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.278928, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.309924, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.294056, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310023, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.278820, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.278756, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.278442, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.311067, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.278148, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310604, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310372, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.311267, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.277562, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.311366, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.311416, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.277307, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310657, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310721, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.293940, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.276095, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.311716, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310920, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310970, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.311304, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.311355, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.274707, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.293434, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.275477, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.311226, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.275074, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.274928, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.274308, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.274636, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.274399, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.274563, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.273712, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.312556, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.273907, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.273761, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.292766, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.272414, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.272261, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.273178, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.311935, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.312913, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.312116, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.292114, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.291868, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.272305, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.272404, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.271480, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.272004, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.271973, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.271829, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.270885, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.271202, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.291894, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.312715, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.271112, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.312814, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.269992, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.270417, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.291143, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.270395, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.313064, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.314001, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.269966, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.269822, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.268802, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.269258, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.290758, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.291124, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.313757, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.313345, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.267909, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.313907, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.314549, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.290815, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.290611, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.267972, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.314749, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.290471, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.313961, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.267398, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.290085, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.290284, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.314161, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.314508, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.315147, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.290331, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.314659, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.290005, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.265849, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.265705, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.314323, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.289558, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.314959, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.263657, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.289872, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.289683, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.264040, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.289719, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.289223, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.315310, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.315909, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.289374, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.263917, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.316095, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.263630, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.263837, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.315067, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.263111, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.289031, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.315205, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.261955, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.288892, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.288846, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.288506, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.262192, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.262420, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.288600, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.262137, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.259780, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.261474, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.288545, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.288125, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.261044, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.316512, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.317142, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.315942, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.316356, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.317257, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.316762, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.259123, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.316172, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.316913, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.316963, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287880, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.259735, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.316806, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287565, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.259313, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287625, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.258609, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.258372, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.258749, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287253, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.257943, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287317, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287097, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.257608, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287239, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.287112, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.317815, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.254770, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.317128, Train accuracy: 0.200000, val accuracy: 0.133333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.254454, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.286856, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.318065, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.317310, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.317355, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.286784, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.317445, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.255653, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.256080, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.255712, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.318466, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.286342, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319105, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.255379, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.317805, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.255099, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.285899, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319355, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319405, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319454, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.286103, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.286057, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.253615, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319683, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319168, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.251940, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.318953, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.285786, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.318432, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319982, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.318521, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319203, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.252082, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.250737, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319669, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319720, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.320330, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319820, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.284947, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.320480, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.284758, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.320553, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.284664, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319802, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.320729, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.284375, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.250774, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.246952, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.250496, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.284333, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.320472, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.320522, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.249940, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.249801, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.320352, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.284050, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.283793, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.248944, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.245165, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.246810, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.284303, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.248691, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.284213, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.320802, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.246052, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.245900, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.247237, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.283435, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.247722, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.246944, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.246802, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.322073, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.322122, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.246378, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.321676, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.283634, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.320532, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.283013, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.283501, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.282804, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.322501, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.322570, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.321752, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.282593, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.243004, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.283190, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.283146, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.244736, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.242392, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.244822, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.244684, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.282169, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.241778, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.282836, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.323201, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.281957, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.322780, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.281851, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.282616, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.281488, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.323515, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.281638, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.282439, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.323650, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.323182, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.281424, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.241549, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.281317, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.321877, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.323950, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.281458, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.241010, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.322042, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.281956, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.324211, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.281868, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.238073, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.323503, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.237762, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.281693, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.239741, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.280939, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.324599, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.236983, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.234093, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.236671, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.236515, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.238565, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.280187, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.324103, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.238332, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.325055, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.238456, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.325155, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.235263, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.324404, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.324791, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.323051, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.323090, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.280821, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.324654, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.231115, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.279426, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.325651, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.325701, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.325747, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.280517, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.325005, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.279667, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.236093, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.232740, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.278621, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.325595, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.235095, Train accuracy: 0.266667, val accuracy: 0.133333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.228797, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.234584, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.234673, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.325506, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.231469, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.231309, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.234564, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.326545, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.278325, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.230670, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.279739, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.233407, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.325957, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.279610, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.226255, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.324290, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.326991, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.277489, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.327091, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.327140, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.277317, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.277260, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.279224, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.326508, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.327394, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.324665, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.224037, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.227772, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.327594, Train accuracy: 0.266667, val accuracy: 0.133333\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-4, num_epochs=500, batch_size=64)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.303272, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302331, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302373, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.299480, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.303393, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.297089, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.300206, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.299783, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.293135, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.298888, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.290224, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.308566, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.288467, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.287742, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.286354, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.310576, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.308904, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.280958, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.310953, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.277543, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.293315, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.312529, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.272315, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.291599, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.269904, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.290610, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.314797, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.289615, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.289178, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.315371, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.317107, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.316403, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.316993, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.286421, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.318837, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.254302, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.250912, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.251599, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.319837, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.320451, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.282912, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.322428, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.245895, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.324635, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.241921, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.238935, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.238951, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.281056, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.325742, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.278270, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.327911, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.327659, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.227579, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.221280, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.276881, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.225347, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.225690, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.274006, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.210729, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.274330, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.274010, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: 2.328071, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.332840, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.274339, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.206022, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.212468, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.212547, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.336743, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.186762, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.206615, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.205323, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.338669, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.339245, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.203680, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.264547, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.339379, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.262833, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.341736, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.262007, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.260696, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.148383, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.189358, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.342840, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.262819, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.328814, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.344449, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.344506, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.260595, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.345774, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.253489, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.326460, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.347402, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.179527, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.324248, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.256576, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.349287, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.262110, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.112430, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.157055, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.102262, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.161104, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.022566, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.087436, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.082337, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.164849, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.984198, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.234852, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.057432, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.356829, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.356278, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.143678, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.225130, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.223661, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.878906, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.127226, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.152071, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.363552, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.239788, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.279069, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.114495, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.122948, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.255656, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.362393, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.261729, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.889107, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.075355, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.364401, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.371331, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.240543, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.816401, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.364965, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.096204, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.375204, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.074912, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.393420, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.162241, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.199093, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.128085, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.254879, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.998462, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.124996, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.154460, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.383762, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.161413, Train accuracy: 0.400000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.511107, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.386871, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.462122, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.144708, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.095381, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.100507, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.255743, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.014334, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.903963, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.112402, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.111549, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.397219, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.219299, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.026721, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.377438, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.400936, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.841428, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.833679, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.104405, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.084696, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.378992, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.963925, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.953646, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.143261, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.978631, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.062284, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.097421, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.865888, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.381208, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.380349, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.941312, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.877569, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.814961, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.382350, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.417419, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.007471, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.383056, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.728897, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.798768, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.895421, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.383906, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.690902, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.422030, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.769833, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.048770, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.831062, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.376168, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.376349, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.376616, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.418569, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.881524, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.299925, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.862783, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.081322, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.427429, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.708568, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.706546, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.079929, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.743752, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.378889, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.224020, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.429424, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.426369, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.113173, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.426621, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.948543, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.946329, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.426977, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.214423, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.670255, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.076244, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.582634, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.928704, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.387980, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.855186, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.386017, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.564714, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.313228, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.388145, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.074503, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.908362, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.198866, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.550961, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.585076, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.542922, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.387689, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.570553, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.534283, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.190587, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.538539, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.426897, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.606404, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.386490, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.327903, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.561491, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.384184, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.385720, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.382357, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.428355, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.385179, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.489756, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.378661, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.424055, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.815450, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.337442, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.477653, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.383267, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.425718, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.336398, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.410480, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.765033, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.381838, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.421258, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.359805, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.358340, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.581512, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.419842, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.437015, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.383860, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.156298, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.383074, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.695407, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.380170, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.378323, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 0.377624, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.333575, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.519893, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.522677, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.403233, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.325825, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.375835, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.331077, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.375252, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.432671, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.331353, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.135042, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.415466, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.302656, Train accuracy: 0.400000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.377142, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.416345, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.295167, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.126408, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.424231, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.544788, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.404951, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.404561, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.033616, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.409830, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.402602, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.428836, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.401343, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.322440, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.384482, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.372380, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.367214, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.458702, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.318980, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.366219, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.373703, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.239840, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.510398, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.424929, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.401203, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.392750, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.019257, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.376859, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.391123, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.343704, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.426460, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.430903, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.066275, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.206829, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.204213, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.201559, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.011276, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.359596, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.384918, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.467315, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.315715, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.004490, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.311802, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.387227, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.424817, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.423058, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.391172, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.421917, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.308897, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.333902, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.380936, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.928652, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.372263, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.032266, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.030221, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.373426, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.889030, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.373243, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.019837, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.351969, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.275680, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.991142, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.271471, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.299018, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.988733, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.368432, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.349325, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.364816, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.253932, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.363616, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.363618, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.336091, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.242403, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.127130, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.416507, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.358731, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.117399, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.233304, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.357439, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.325895, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.364638, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.069318, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.343556, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.367009, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.315369, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.348261, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.267596, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.341668, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.344225, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.078394, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.202070, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 0.418241, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 0.605342, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.045159, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.348074, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 0.268039, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.338270, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.298881, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.022486, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 0.996774, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 0.359545, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.343694, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.014529, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.335491, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.522764, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.419472, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.279937, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.284080, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.284546, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.282276, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.418809, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.231292, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.275468, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.274986, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.272826, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.306345, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.334100, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.934712, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.269577, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.270746, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.921993, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.258916, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.213391, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.251687, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.225403, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.222523, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.106369, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.889349, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.422706, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.746749, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.933617, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.951844, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.180127, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.274253, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.423693, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.199722, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.321251, Train accuracy: 0.600000, val accuracy: 0.066667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.077111, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.193966, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.069472, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.936175, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.337936, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.208263, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.307186, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.922200, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.412572, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.314787, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.611795, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.917702, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.917814, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.312523, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.185701, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.556411, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.315199, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.179573, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.309368, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.760337, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.251288, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.126708, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.312522, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.410610, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.409728, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.161007, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.304146, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.324223, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.302874, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.434918, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.471326, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.710175, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.416807, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.105787, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.104940, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.898580, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.449370, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.896819, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.295968, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.193012, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.894180, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.109645, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.892531, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.865416, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.411566, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.864651, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.266542, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.087729, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.382409, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.301805, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.374054, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.863442, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.856070, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.161321, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.127744, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.843893, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.077855, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.153003, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.023079, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.415901, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.279676, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.287400, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.323115, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.070238, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.800631, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.062677, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.869213, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.579375, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.065984, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.405517, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.005391, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.975817, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.863230, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.820795, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.018866, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.405710, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.058598, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.286179, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.105554, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.906296, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 2.284620, Train accuracy: 0.733333, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 128, reg = 1e-3)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, num_epochs=500, batch_size=5)\n",
    "#MomentumSGD()\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.468025, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 1.924924, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.527588, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.556937, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.856964, Train accuracy: 0.224778, val accuracy: 0.227000\n",
      "Loss: 2.127242, Train accuracy: 0.259222, val accuracy: 0.259000\n",
      "Loss: 2.558817, Train accuracy: 0.296111, val accuracy: 0.304000\n",
      "Loss: 1.109771, Train accuracy: 0.351778, val accuracy: 0.354000\n",
      "Loss: 1.905924, Train accuracy: 0.410000, val accuracy: 0.401000\n",
      "Loss: 0.245075, Train accuracy: 0.466444, val accuracy: 0.466000\n",
      "Loss: 1.846441, Train accuracy: 0.513111, val accuracy: 0.514000\n",
      "Loss: 3.349033, Train accuracy: 0.534333, val accuracy: 0.520000\n",
      "Loss: 2.093285, Train accuracy: 0.589000, val accuracy: 0.583000\n",
      "Loss: 1.460751, Train accuracy: 0.608889, val accuracy: 0.591000\n",
      "Loss: 0.084197, Train accuracy: 0.644556, val accuracy: 0.630000\n",
      "Loss: 1.358462, Train accuracy: 0.658333, val accuracy: 0.643000\n",
      "Loss: 1.844085, Train accuracy: 0.677333, val accuracy: 0.667000\n",
      "Loss: 0.975072, Train accuracy: 0.685667, val accuracy: 0.666000\n",
      "Loss: 1.069272, Train accuracy: 0.699778, val accuracy: 0.682000\n",
      "Loss: 1.324072, Train accuracy: 0.711222, val accuracy: 0.693000\n",
      "Loss: 0.521132, Train accuracy: 0.713889, val accuracy: 0.697000\n",
      "Loss: 2.170822, Train accuracy: 0.728778, val accuracy: 0.707000\n",
      "Loss: 0.904770, Train accuracy: 0.732222, val accuracy: 0.709000\n",
      "Loss: 3.833618, Train accuracy: 0.746333, val accuracy: 0.700000\n",
      "Loss: 0.297177, Train accuracy: 0.752889, val accuracy: 0.710000\n",
      "Loss: 0.243630, Train accuracy: 0.751111, val accuracy: 0.705000\n",
      "Loss: 0.557031, Train accuracy: 0.762111, val accuracy: 0.704000\n",
      "Loss: 0.647222, Train accuracy: 0.764333, val accuracy: 0.718000\n",
      "Loss: 0.104065, Train accuracy: 0.776222, val accuracy: 0.713000\n",
      "Loss: 3.217346, Train accuracy: 0.780778, val accuracy: 0.734000\n",
      "Loss: 0.962186, Train accuracy: 0.781778, val accuracy: 0.717000\n",
      "Loss: 0.146490, Train accuracy: 0.796667, val accuracy: 0.729000\n",
      "Loss: 0.163194, Train accuracy: 0.801222, val accuracy: 0.730000\n",
      "Loss: 0.936715, Train accuracy: 0.801778, val accuracy: 0.720000\n",
      "Loss: 0.563495, Train accuracy: 0.804667, val accuracy: 0.738000\n",
      "Loss: 1.200177, Train accuracy: 0.814111, val accuracy: 0.740000\n",
      "Loss: 1.087403, Train accuracy: 0.815222, val accuracy: 0.729000\n",
      "Loss: 0.173149, Train accuracy: 0.819667, val accuracy: 0.734000\n",
      "Loss: 0.629074, Train accuracy: 0.827000, val accuracy: 0.747000\n",
      "Loss: 0.141406, Train accuracy: 0.836556, val accuracy: 0.740000\n",
      "Loss: 0.170774, Train accuracy: 0.831444, val accuracy: 0.727000\n",
      "Loss: 0.191094, Train accuracy: 0.846000, val accuracy: 0.749000\n",
      "Loss: 0.430366, Train accuracy: 0.841444, val accuracy: 0.743000\n",
      "Loss: 0.333094, Train accuracy: 0.841667, val accuracy: 0.749000\n",
      "Loss: 0.151468, Train accuracy: 0.843667, val accuracy: 0.742000\n",
      "Loss: 0.777336, Train accuracy: 0.858889, val accuracy: 0.756000\n",
      "Loss: 0.188896, Train accuracy: 0.852111, val accuracy: 0.752000\n",
      "Loss: 1.328465, Train accuracy: 0.861222, val accuracy: 0.746000\n",
      "Loss: 0.352099, Train accuracy: 0.849667, val accuracy: 0.732000\n",
      "Loss: 0.170922, Train accuracy: 0.871000, val accuracy: 0.753000\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rate = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 50\n",
    "batch_size = 40\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "    \n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "\n",
    "model = TwoLayerNet(n_input=train_X.shape[1], n_output=10, \n",
    "                       hidden_layer_size=hidden_layer_size, \n",
    "                       reg=reg_strength)\n",
    "\n",
    "trainer = Trainer(model, dataset, MomentumSGD(),\n",
    "                        learning_rate=learning_rate, \n",
    "                        learning_rate_decay=learning_rate_decay, \n",
    "                        num_epochs=num_epochs, \n",
    "                        batch_size=batch_size)\n",
    "\n",
    "loss_history, train_accuracy, val_accuracy = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved: 0.756000\n"
     ]
    }
   ],
   "source": [
    "best_classifier = model\n",
    "best_val_accuracy = val_accuracy[np.argmax(val_accuracy)]\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fadc2e21220>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.723000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
